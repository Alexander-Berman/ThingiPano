{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import concurrent\n",
    "import concurrent.futures\n",
    "\n",
    "from keras.applications import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Reshape, MaxPooling1D, Flatten, Concatenate\n",
    "\n",
    "from keras.preprocessing.image import image, img_to_array, load_img, ImageDataGenerator\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function for Cropping Image by thirds\n",
    "def crop(im, height, width):\n",
    "    imgwidth, imgheight = im.size\n",
    "    rows = np.int(imgheight/height)\n",
    "    cols = np.int(imgwidth/width)\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            box = (j*width, i*height, (j+1)*width, (i+1)*height)\n",
    "            yield im.crop(box)\n",
    "            \n",
    "#Custom generator that takes in a pandas dataframe with file paths,\n",
    "#    allowing training/predicting without loading all files into directory\n",
    "def generator_from_df(df, img_dir, batch_size, pano_size, img_size, shuffle=False):\n",
    "    nbatches, n_skipped_per_epoch = divmod(df.shape[0], batch_size)\n",
    "    \n",
    "    if shuffle:\n",
    "        df = df.sample(frac=1) #shuffles the dataframe\n",
    "        \n",
    "    while 1:\n",
    "        for i in range(nbatches):\n",
    "            j = i*batch_size+batch_size\n",
    "            \n",
    "            sub = df.iloc[i:j]\n",
    "            mask = np.zeros(sub.columns.shape,dtype=bool)\n",
    "            mask[0]=1\n",
    "            img_files = [f[0] for f in sub[sub.columns[mask]].values]\n",
    "            try:\n",
    "                out_images = [[],[],[]] #three\n",
    "                for img_file in img_files:  \n",
    "                    im = load_img(os.path.join(img_dir,img_file))\n",
    "                    imgwidth, imgheight = im.size\n",
    "                    height = np.int(imgheight//3)#split into thirds\n",
    "                    \n",
    "                    start_num = 0\n",
    "                    for k, piece in enumerate(crop(im, height, imgwidth), start_num):\n",
    "                        img = Image.new('RGB', (imgwidth, height), 255)\n",
    "                        img.paste(piece)\n",
    "                        img = img.resize((img_size[1],img_size[0])) #may change depending on your environment\n",
    "                        out_images[k].append(preprocess_input(img_to_array(img)))\n",
    "                \n",
    "                yield out_images\n",
    "\n",
    "            except IOError as err:\n",
    "                print(\"ERROR!\",err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "pano_1 (InputLayer)             (None, 125, 260, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pano_2 (InputLayer)             (None, 125, 260, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pano_3 (InputLayer)             (None, 125, 260, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "base_model (Model)              (None, 1, 6, 2048)   21802784    pano_1[0][0]                     \n",
      "                                                                 pano_2[0][0]                     \n",
      "                                                                 pano_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 1, 6, 6144)   0           base_model[1][0]                 \n",
      "                                                                 base_model[2][0]                 \n",
      "                                                                 base_model[3][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 36864)        0           concatenate_6[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 21,802,784\n",
      "Trainable params: 0\n",
      "Non-trainable params: 21,802,784\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Example for creating a keras model that can use the previous data generator for panoramas.\n",
    "#  The inception model can be changed to other architectures.\n",
    "#  After the row-wise max-pooling, you can add more layers for custom classification models.\n",
    "\n",
    "input_shape=(125, 260, 3) #Can be adjusted, up to 360x751 (size of each panorama individually)\n",
    "#This input pretends that it is RGB for use with pre-trained models.\n",
    "#  I would recommend changing it to one channel (grayscale) when training models from the ground-up\n",
    "#  To do this, change the input shape from 3 to 1, and 'RGB' in the generator to 'L'\n",
    "\n",
    "def create_model(input_shape):\n",
    "    base_input = Input(shape=input_shape)\n",
    "    #using pre-trained model. \n",
    "    #  Change as you see fit to train from ground-up or try different architectures\n",
    "    app_model = InceptionV3(weights = 'imagenet',  \n",
    "                      include_top = False, \n",
    "                      input_tensor = base_input,\n",
    "                      input_shape = input_shape,\n",
    "                      pooling = None)\n",
    "    net = app_model.output\n",
    "\n",
    "    #---- Row-Wise Max-Pooling (Pooling Panorama as a Cylinder where x-axis loops)\n",
    "    w,h,c = [int(v) for v in app_model.output.shape[1:]] #these values change with INPUT_SHAPE OR APP_MODEL\n",
    "    net = Reshape((w, h*c))(net)\n",
    "    net = MaxPooling1D()(net)\n",
    "    net = Reshape((int(w/2), h, c))(net)\n",
    "    #---- \n",
    "    base_model = Model(app_model.input, net, name=\"base_model\")\n",
    "    \n",
    "    #Now combining all into one network that produces features for all three views\n",
    "    input_p1 = Input(shape=input_shape, name='pano_1')\n",
    "    input_p2 = Input(shape=input_shape, name='pano_2')\n",
    "    input_p3 = Input(shape=input_shape, name='pano_3')\n",
    "    inputs = [input_p1,input_p2,input_p3]\n",
    "    \n",
    "    #flatten into one giant vector\n",
    "    outputs = [None]*len(inputs)\n",
    "    for i,input_i in enumerate(inputs):\n",
    "        outputs[i] = base_model(input_i)\n",
    "    out_vectors = Concatenate(axis=-1)(outputs)\n",
    "    out_vectors = Flatten()(out_vectors) \n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=out_vectors, name='Panorama_Model')\n",
    "    #keras predict is not always thread-safe unless you freeze and compile\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy') #not actually training\n",
    "    return model\n",
    "\n",
    "model = create_model(input_shape)\n",
    "model._make_predict_function()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PANO_DIR = '<PATH TO PANORMA DIR>' \n",
    "\n",
    "panorama_files = []#list of all panorama files\n",
    "for thingID in list(os.listdir(os.path.join(PANO_DIR))):\n",
    "    #thingID corresponds to file in https://thingiverse.com/thing:<thingID>\n",
    "    thing_files = list()\n",
    "    for filename in os.listdir(os.path.join(PANO_DIR,thingID)): \n",
    "        if filename.endswith('.png'):\n",
    "            thing_files.append(os.path.join(thingID,filename))\n",
    "    panorama_files.extend(list(set(thing_files)))\n",
    "    \n",
    "print(\"Found {} Panoramas\".format(len(panorama_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Example Script to generate features for all renders in ThingiPano. takes a couple days via CPU\n",
    "\n",
    "pano_feature_file = 'pano_vecs.csv' #path to csv to write all features (will be giant ~300GB)\n",
    "num_threads = 1 #number of threads to predict on\n",
    "\n",
    "with open(pano_feature_file,'w') as out_f:\n",
    "    out_f.write(\"item_id,file_name,image_features\\n\")\n",
    "    \n",
    "    chunk_size = 128\n",
    "    num_chunks = len(panorama_files)//chunk_size +1\n",
    "    \n",
    "    panorama_files_chunks = []\n",
    "    for i in range(num_chunks):\n",
    "        if chunk_size*i>=len(panorama_files):\n",
    "            break\n",
    "        top_range = chunk_size*i+chunk_size\n",
    "        if top_range > len(panorama_files):\n",
    "            top_range = len(panorama_files)\n",
    "        panorama_files_chunks.append(pd.DataFrame(panorama_files[chunk_size*i:top_range]))\n",
    "    print(\"Running on {} Chunks\".format(len(panorama_files_chunks)))\n",
    "\n",
    "    def write_pano_features(df):\n",
    "        datagen = generator_from_df(df,PANO_DIR,1,input_shape)\n",
    "        pano_features = model.predict_generator(datagen,len(df[0].tolist()),verbose=0,workers=1,use_multiprocessing=True)\n",
    "        \n",
    "        item_ids =    [fn.split('/')[0] for fn in df[0].tolist()]\n",
    "        thing_names = ['/'.join(fn.split('/')[1:]) for fn in df[0].tolist()] #put in quotes as some filenames have commas\n",
    "        for j in range(len(pano_features)):\n",
    "            out_f.write(item_ids[j]+',\"'+thing_names[j]+'\",\"['+\",\".join([str(ft) for ft in pano_features[j]])+']\"\\n')\n",
    "        return True  \n",
    " \n",
    "    with concurrent.futures.ThreadPoolExecutor(num_threads) as executor: #increase the number to use more than one thread\n",
    "        for i,data_part in enumerate(zip(panorama_files_chunks,executor.map(write_pano_features,panorama_files_chunks))):\n",
    "            if i%10==0:\n",
    "                print(\"chunks processed: {}\\t{}%\".format(i,100.0*i/len(panorama_files_chunks)))\n",
    "            pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
